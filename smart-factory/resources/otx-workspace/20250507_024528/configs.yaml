data_root: /home/garamsong/workspace/smart-factory/resources/bottlecap
work_dir: /home/garamsong/workspace/smart-factory/resources/otx-workspace
callback_monitor: val/accuracy
disable_infer_num_classes: false
engine:
  task: MULTI_CLASS_CLS
  device: auto
  num_devices: 1
data:
  task: MULTI_CLASS_CLS
  data_format: imagenet_with_subset_dirs
  train_subset:
    batch_size: 64
    subset_name: train
    transforms:
    - class_path: otx.core.data.transform_libs.torchvision.RandomResizedCrop
      init_args:
        scale: $(input_size)
    - class_path: otx.core.data.transform_libs.torchvision.PhotoMetricDistortion
      enable: false
    - class_path: otx.core.data.transform_libs.torchvision.RandomAffine
      enable: false
    - class_path: otx.core.data.transform_libs.torchvision.RandomFlip
      init_args:
        prob: 0.5
        is_numpy_to_tvtensor: true
    - class_path: torchvision.transforms.v2.RandomVerticalFlip
      enable: false
    - class_path: torchvision.transforms.v2.GaussianBlur
      enable: false
      init_args:
        kernel_size: 5
    - class_path: torchvision.transforms.v2.ToDtype
      init_args:
        dtype: ${as_torch_dtype:torch.float32}
        scale: false
    - class_path: torchvision.transforms.v2.Normalize
      init_args:
        mean:
        - 123.675
        - 116.28
        - 103.53
        std:
        - 58.395
        - 57.12
        - 57.375
    - class_path: torchvision.transforms.v2.GaussianNoise
      enable: false
    transform_lib_type: TORCHVISION
    num_workers: 2
    sampler:
      class_path: otx.algo.samplers.balanced_sampler.BalancedSampler
      init_args: {}
    to_tv_image: false
  val_subset:
    batch_size: 64
    subset_name: val
    transforms:
    - class_path: otx.core.data.transform_libs.torchvision.Resize
      init_args:
        scale: $(input_size)
        is_numpy_to_tvtensor: true
    - class_path: torchvision.transforms.v2.ToDtype
      init_args:
        dtype: ${as_torch_dtype:torch.float32}
        scale: false
    - class_path: torchvision.transforms.v2.Normalize
      init_args:
        mean:
        - 123.675
        - 116.28
        - 103.53
        std:
        - 58.395
        - 57.12
        - 57.375
    transform_lib_type: TORCHVISION
    num_workers: 2
    sampler:
      class_path: torch.utils.data.RandomSampler
      init_args: {}
    to_tv_image: false
  test_subset:
    batch_size: 64
    subset_name: test
    transforms:
    - class_path: otx.core.data.transform_libs.torchvision.Resize
      init_args:
        scale: $(input_size)
        is_numpy_to_tvtensor: true
    - class_path: torchvision.transforms.v2.ToDtype
      init_args:
        dtype: ${as_torch_dtype:torch.float32}
        scale: false
    - class_path: torchvision.transforms.v2.Normalize
      init_args:
        mean:
        - 123.675
        - 116.28
        - 103.53
        std:
        - 58.395
        - 57.12
        - 57.375
    transform_lib_type: TORCHVISION
    num_workers: 2
    sampler:
      class_path: torch.utils.data.RandomSampler
      init_args: {}
    to_tv_image: false
  tile_config:
    enable_tiler: false
    enable_adaptive_tiling: true
    tile_size:
    - 400
    - 400
    overlap: 0.2
    iou_threshold: 0.45
    max_num_instances: 1500
    object_tile_ratio: 0.03
    sampling_ratio: 1.0
    with_full_img: false
  mem_cache_size: 1GB
  mem_cache_img_max_size:
  - 500
  - 500
  image_color_channel: RGB
  include_polygons: false
  ignore_index: 255
  unannotated_items_ratio: 0.0
  auto_num_workers: false
  input_size:
  - 224
  - 224
  input_size_multiplier: 1
checkpoint: /home/garamsong/workspace/smart-factory/resources/otx-workspace/20250507_024454/best_checkpoint.ckpt
export_format: OPENVINO
export_precision: FP32
explain: false
export_demo_package: false
max_epochs: 90
model:
  class_path: otx.algo.classification.multiclass_models.MobileNetV3MulticlassCls
  init_args:
    data_input_params:
      input_size:
      - 224
      - 224
      mean:
      - 123.675
      - 116.28
      - 103.53
      std:
      - 58.395
      - 57.12
      - 57.375
    model_name: mobilenetv3_large
    freeze_backbone: false
    metric: otx.core.metrics.accuracy._multi_class_cls_metric_callable
    torch_compile: false
